{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd432ab-f41c-4c1a-bfd0-b5e7ed275f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (8.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from plotly) (1.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from plotly) (24.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (9.6.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\SNEHA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\SNEHA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\SNEHA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn joblib plotly ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2a8d5f-7d13-4599-8ed0-4bb8d82ea1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-genai\n",
      "  Downloading google_genai-1.41.0-py3-none-any.whl.metadata (45 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-genai) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-genai) (2.40.1)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-genai) (2.11.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-genai) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-genai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.4.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\sneha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
      "Downloading google_genai-1.41.0-py3-none-any.whl (245 kB)\n",
      "Installing collected packages: google-genai\n",
      "Successfully installed google-genai-1.41.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\SNEHA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\SNEHA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\SNEHA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd6c67f-b740-4b57-80e8-fec50849cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Imported Successfully ---\n",
      "Initial shape: (9564, 49)\n",
      "2. Shape after cleaning missing values: (9201, 7)\n",
      "\n",
      "--- Data Preparation Complete. Ready for MLP Training ---\n",
      "X_train_scaled shape: (7360, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# --- 1. Data Import (Kepler/KOI Data) ---\n",
    "# We use skiprows=53 to bypass the metadata at the top of the NASA CSV file.\n",
    "KEPLER_FILE = \"cumulative_2025.10.03_00.23.38.csv\"\n",
    "SKIP_ROWS = 53\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(KEPLER_FILE, skiprows=SKIP_ROWS)\n",
    "\n",
    "print(\"--- 1. Data Imported Successfully ---\")\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "# --- 2. Feature and Target Selection (Simplification for MVP MLP) ---\n",
    "# TARGET: koi_pdisposition (Disposition using Kepler Data: CANDIDATE or FALSE POSITIVE)\n",
    "# FEATURES: A concise set of key physical/observable parameters.\n",
    "\n",
    "TARGET_COLUMN = 'koi_pdisposition'\n",
    "FEATURE_COLUMNS = [\n",
    "    'koi_period',     # Orbital Period [days]\n",
    "    'koi_prad',       # Planet Radius [Earth radii]\n",
    "    'koi_teq',        # Equilibrium Temperature [K]\n",
    "    'koi_duration',   # Transit Duration [hrs]\n",
    "    'koi_impact',     # Impact Parameter\n",
    "    'koi_insol',      # Insolation Flux [Earth flux]\n",
    "]\n",
    "\n",
    "# Select necessary columns\n",
    "df_model = df[[TARGET_COLUMN] + FEATURE_COLUMNS].copy()\n",
    "\n",
    "# --- 3. Data Cleaning (Crucial Step for ML) ---\n",
    "# For MVP simplicity, we drop rows with ANY missing values in the selected columns.\n",
    "# (In a real scenario, you might use imputation, but dropping is faster here.)\n",
    "df_model.dropna(inplace=True)\n",
    "\n",
    "print(f\"2. Shape after cleaning missing values: {df_model.shape}\")\n",
    "\n",
    "# --- 4. Target Transformation (Creating Binary Y) ---\n",
    "# Convert the categorical target ('CANDIDATE'/'FALSE POSITIVE') to binary (1/0)\n",
    "df_model['y'] = df_model[TARGET_COLUMN].apply(lambda x: 1 if x == 'CANDIDATE' else 0)\n",
    "\n",
    "# --- 5. Data Splitting and Scaling (Prep for MLP) ---\n",
    "X = df_model[FEATURE_COLUMNS]\n",
    "y = df_model['y']\n",
    "\n",
    "# Split data into 80% training and 20% testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the data (StandardScaler is mandatory for MLP/Neural Networks performance)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- 6. MLP Model Setup (Ready for Training) ---\n",
    "# Setup a simple Multi-Layer Perceptron (MLP) Classifier:\n",
    "# - Two hidden layers with 16 neurons each: (16, 16)\n",
    "# - Max iterations set to 500 for stable convergence.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(16, 16), max_iter=500, random_state=42)\n",
    "\n",
    "print(\"\\n--- Data Preparation Complete. Ready for MLP Training ---\")\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953c4fef-88fa-4cab-b4a8-675a1ead4f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Model Accuracy on Test Set: 0.8121\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.77      0.80       918\n",
      "           1       0.79      0.86      0.82       923\n",
      "\n",
      "    accuracy                           0.81      1841\n",
      "   macro avg       0.81      0.81      0.81      1841\n",
      "weighted avg       0.81      0.81      0.81      1841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the MLP model\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate model performance (Crucial for judging the 'Working' part of MVP)\n",
    "accuracy = mlp.score(X_test_scaled, y_test)\n",
    "print(f\"MLP Model Accuracy on Test Set: {accuracy:.4f}\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc54d79-ac41-47ce-8ba7-68fc739a42d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model and Scaler saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model and the scaler object\n",
    "joblib.dump(mlp, 'mlp_exoplanet_model.pkl')\n",
    "joblib.dump(scaler, 'scaler_object.pkl')\n",
    "print(\"\\nModel and Scaler saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de015cc8-be7f-44c0-8fa6-9f093cd42b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MLP training...\n",
      "MLP Training Complete.\n",
      "\n",
      "Model Accuracy on Test Set: 0.8121\n",
      "\n",
      "Classification Report (Key Metric for Judges):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.77      0.80       918\n",
      "           1       0.79      0.86      0.82       923\n",
      "\n",
      "    accuracy                           0.81      1841\n",
      "   macro avg       0.81      0.81      0.81      1841\n",
      "weighted avg       0.81      0.81      0.81      1841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 7: Train the MLP Model ---\n",
    "# This is the training process itself. It should run quickly since we used a simple model.\n",
    "print(\"Starting MLP training...\")\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "print(\"MLP Training Complete.\")\n",
    "\n",
    "# --- STEP 8: Evaluate Model Performance (Check the 'Working' part of the MVP) ---\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Make predictions on the unseen test set\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "y_prob = mlp.predict_proba(X_test_scaled)[:, 1] # Probability of being a 'CANDIDATE' (class 1)\n",
    "\n",
    "# Report results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy on Test Set: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report (Key Metric for Judges):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Note: Focus on the 'precision' and 'recall' for class 1 (CANDIDATE) \n",
    "# as this shows how well the model finds true exoplanets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "847829c1-a7a0-43aa-b367-897ee22b624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AI-Identified High-Confidence Candidates (Top 5) ---\n",
      "     koi_pdisposition  koi_period  koi_prad  koi_teq  koi_duration  \\\n",
      "9486   FALSE POSITIVE   14.340036      0.68    248.0         5.700   \n",
      "6245   FALSE POSITIVE   40.880154      0.92    265.0         5.472   \n",
      "6913   FALSE POSITIVE   24.500270      0.73    409.0         4.079   \n",
      "2963   FALSE POSITIVE   25.920009      1.28    515.0         5.609   \n",
      "6240   FALSE POSITIVE   37.078668      0.80    419.0         4.403   \n",
      "\n",
      "      koi_impact  koi_insol  y  confidence  \n",
      "9486      0.0060       0.89  0    0.911346  \n",
      "6245      0.0160       1.17  0    0.908679  \n",
      "6913      0.4050       6.61  0    0.907768  \n",
      "2963      0.0540      16.59  0    0.906995  \n",
      "6240      0.5101       7.27  0    0.903501  \n",
      "\n",
      "Candidate list saved to: ai_identified_candidates.csv\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 9: Identify New Candidates ---\n",
    "# Re-run prediction on the *original full, cleaned dataframe* to get confidence scores for all objects.\n",
    "\n",
    "# 1. Scale the full cleaned feature set\n",
    "X_all_scaled = scaler.transform(X) # Use the same scaler object trained previously\n",
    "\n",
    "# 2. Get the probability of being a CANDIDATE (class 1)\n",
    "df_model['confidence'] = mlp.predict_proba(X_all_scaled)[:, 1]\n",
    "\n",
    "# 3. Filter for candidates with high confidence but not yet confirmed\n",
    "# We look for objects currently labeled as 'FALSE POSITIVE' or those we want to re-examine, \n",
    "# where the AI confidence is high (e.g., > 90% confidence).\n",
    "new_candidates = df_model[\n",
    "    (df_model['koi_pdisposition'] == 'FALSE POSITIVE') & \n",
    "    (df_model['confidence'] >= 0.90)\n",
    "].sort_values(by='confidence', ascending=False)\n",
    "\n",
    "print(\"\\n--- AI-Identified High-Confidence Candidates (Top 5) ---\")\n",
    "print(new_candidates.head(5))\n",
    "\n",
    "# --- 10. Save Candidate List for the Web App ---\n",
    "# This file will be the primary data source for your Streamlit/Flask app visualization.\n",
    "candidates_file_name = 'ai_identified_candidates.csv'\n",
    "new_candidates.to_csv(candidates_file_name, index=False)\n",
    "print(f\"\\nCandidate list saved to: {candidates_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20559c17-5681-449a-8c18-6807f51c47c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model ('mlp_exoplanet_model.pkl') and Scaler ('scaler_object.pkl') saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 11: Save Model and Scaler ---\n",
    "# Import the joblib library (you should have done this in the first step's imports)\n",
    "import joblib \n",
    "\n",
    "# Save the trained MLP model\n",
    "joblib.dump(mlp, 'mlp_exoplanet_model.pkl')\n",
    "\n",
    "# Save the scaler object (MANDATORY, as your web app must scale new data before prediction)\n",
    "joblib.dump(scaler, 'scaler_object.pkl')\n",
    "\n",
    "print(\"\\nModel ('mlp_exoplanet_model.pkl') and Scaler ('scaler_object.pkl') saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6a240-23ac-4d9d-90c9-b9948c2783d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
